{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gardener on Metal \u00b6 The Cloud Native IaaS \u00b6 Overview \u00b6 Gardener on Metal (short: onmetal ) is a Cloud Native IaaS provider for your Private Cloud - licensed under Apache License v2.0 and other open source licenses. Gardener on Metal aims to be the best deployment target for Cloud Native Applications. If your application runs with Kubernetes and you want to deploy it on your own hardware, Gardener on Metal is your IaaS provider of choice! We love Kubernetes! Every part of the phyiscal infrastructure is managed using Kubernetes: Servers, Switches, Routers, NICs. By using Kubernetes as the control plane for compute, network, storage and applications you can reduce the complexity of your technology stack significantly. If your SREs already know how to manage applications with Kubernetes they will quite fast get the hang of a Kubernetes managed cloud infrastructure. Gardener on Metal provides different infrastructure components you already know from hyperscalers: Kubernetes Clusters ( Gardener managed) Bare Metal Machines Virtual Machines Block Storage ( Ceph based) Object Storage ( Ceph based, S3 compatible) Virtual Networks (including true IPv4/IPv6 Dual-Stack support) Load Balancers NAT Gateways Priorities \u00b6 Stability & Resiliency Security Innovation Technical Debt Gardener on Metal focuses on providing a stable and secure infrastructure. This does not always work with seasoned approaches. Here the innovation comes in! We have to evaluate existing technologies and think about, does it maybe make sense to create something new to reach our stability and security goals? Gardener on Metal is not a new Open Stack \u00b6 The target of Gardener on Metal is not to virtualize or consolidate your existing on-prem infrastructure, but to be a slim deployment target for your cloud native workload. This allows us to use some short-cuts. For example onmetal's virtual networks have no support for Layer 2 Ethernet broadcasts. Instead the networks are Layer 3 only. This means all traffic between the instances is routed. The infrastructure therefore does not need to manage IP to MAC address resolutions, caching and cache invalidation. There is no need to care about multiplying packets of broadcasts, which could lead to micro-congestions and noisy-neighbor issues anymore. Gardener on Metal does not support live migration of VMs \u00b6 Instead a cloud native application follows the cloud native principle, that single service instances are not handled like pets but like cattle. This means that any part of a Cloud Native Application may be terminated without impact of service availability. Instead of live migration, Gardener on Metal sends SIGKILL signals to the application so it can be removed from the load balancer and re-deployed on a different node. This may not be quite that easy with applications that have a large memory footprint due to large caches - on the other hand those applications also have issues when being live migrated. CPU Architectures \u00b6 Gardener on Metal is tested on Intel and AMD x86_64 processors. Due to its modular architecture Gardener on Metal should be easily portable to ARM64 (AArch64) or RISC-V processors. This is on our roadmap but we have not set a deadline, yet. Network \u00b6 The whole network control plane is part of Gardener on Metal. All physical network switches are part of a Kubernetes infrastructure cluster. We use Kubernetes operators for configuring and monitoring the switches. The Switch OS is SONiC - an open source network operating system initially developed by Microsoft. Nowadays a lot of different switch vendors contribute to SONiC. For high speed and low latency network access Gardener on Metal leverages newest NIC technology. With DPDK and rte_flow we are able to make use of a NIC's hardware acceleration while staying very flexible thanks to userspace network programming. SmartNICs (aka DPUs) are used for bare metal deployments to isolate different customers. Storage \u00b6 Most applications need state. Gardener on Metal provides state via Block Storage and an S3-compatible Object Storage. We have forked the Rook-Ceph-Operator and adapted it to fit our production requirements. Most modifications are about day-2 operations of Ceph clusters. The target is to automate as much as possible of Ceph operations. Storage is hard. We want to make easier and available for everyone. Gardener on Metal in a first step does not adopt the hyperconverged infrastructure philosophy. We build clusters with dedicated compute and storage nodes. This allows us to scale both parts independent from another. Also storage is not always the same. You may want to change the ratio of NVMe space to HDD space. This allows to optimize according to your workload. If you need more HDD space for your datalake or your backups, just add more servers with spinning disks. Gardener on Metal's Ceph operator cares about integrating the new storage nodes into the cluster. Compute \u00b6 Compute comes in different flavors with Gardener on Metal: As a bare metal instance or as a virtual machine. A bare metal instance is also often called a dedicated server or root server . The customer has full access to the server's hardware. We use SmartNICs aka DPUs to isolate a physical server from other customers. Using a DPU the bare metal instance can consume additional managed services like managed Firewall, NAT gateways or network block devices. A Virtual Machine is in the end also just a Linux process . We deploy virtual machines via Kubernetes to a set of servers that act as hypervisors.The customer gets root access to the VM and can do whatever he wants with the VM. Kubernetes comes as a Service with Gardener on Metal. The customer does not need to care about bare metal or VM administrations when a Gardener managed Kubernetes cluster is used. Gardener will care about installing all required packages and Kubernetes artifacts. With the provided kubeconfig customers can connect to their Kubernetes clusters within minutes.","title":"Code of Conduct"},{"location":"#gardener-on-metal","text":"","title":"Gardener on Metal"},{"location":"#the-cloud-native-iaas","text":"","title":"The Cloud Native IaaS"},{"location":"#overview","text":"Gardener on Metal (short: onmetal ) is a Cloud Native IaaS provider for your Private Cloud - licensed under Apache License v2.0 and other open source licenses. Gardener on Metal aims to be the best deployment target for Cloud Native Applications. If your application runs with Kubernetes and you want to deploy it on your own hardware, Gardener on Metal is your IaaS provider of choice! We love Kubernetes! Every part of the phyiscal infrastructure is managed using Kubernetes: Servers, Switches, Routers, NICs. By using Kubernetes as the control plane for compute, network, storage and applications you can reduce the complexity of your technology stack significantly. If your SREs already know how to manage applications with Kubernetes they will quite fast get the hang of a Kubernetes managed cloud infrastructure. Gardener on Metal provides different infrastructure components you already know from hyperscalers: Kubernetes Clusters ( Gardener managed) Bare Metal Machines Virtual Machines Block Storage ( Ceph based) Object Storage ( Ceph based, S3 compatible) Virtual Networks (including true IPv4/IPv6 Dual-Stack support) Load Balancers NAT Gateways","title":"Overview"},{"location":"#priorities","text":"Stability & Resiliency Security Innovation Technical Debt Gardener on Metal focuses on providing a stable and secure infrastructure. This does not always work with seasoned approaches. Here the innovation comes in! We have to evaluate existing technologies and think about, does it maybe make sense to create something new to reach our stability and security goals?","title":"Priorities"},{"location":"#gardener-on-metal-is-not-a-new-open-stack","text":"The target of Gardener on Metal is not to virtualize or consolidate your existing on-prem infrastructure, but to be a slim deployment target for your cloud native workload. This allows us to use some short-cuts. For example onmetal's virtual networks have no support for Layer 2 Ethernet broadcasts. Instead the networks are Layer 3 only. This means all traffic between the instances is routed. The infrastructure therefore does not need to manage IP to MAC address resolutions, caching and cache invalidation. There is no need to care about multiplying packets of broadcasts, which could lead to micro-congestions and noisy-neighbor issues anymore.","title":"Gardener on Metal is not a new Open Stack"},{"location":"#gardener-on-metal-does-not-support-live-migration-of-vms","text":"Instead a cloud native application follows the cloud native principle, that single service instances are not handled like pets but like cattle. This means that any part of a Cloud Native Application may be terminated without impact of service availability. Instead of live migration, Gardener on Metal sends SIGKILL signals to the application so it can be removed from the load balancer and re-deployed on a different node. This may not be quite that easy with applications that have a large memory footprint due to large caches - on the other hand those applications also have issues when being live migrated.","title":"Gardener on Metal does not support live migration of VMs"},{"location":"#cpu-architectures","text":"Gardener on Metal is tested on Intel and AMD x86_64 processors. Due to its modular architecture Gardener on Metal should be easily portable to ARM64 (AArch64) or RISC-V processors. This is on our roadmap but we have not set a deadline, yet.","title":"CPU Architectures"},{"location":"#network","text":"The whole network control plane is part of Gardener on Metal. All physical network switches are part of a Kubernetes infrastructure cluster. We use Kubernetes operators for configuring and monitoring the switches. The Switch OS is SONiC - an open source network operating system initially developed by Microsoft. Nowadays a lot of different switch vendors contribute to SONiC. For high speed and low latency network access Gardener on Metal leverages newest NIC technology. With DPDK and rte_flow we are able to make use of a NIC's hardware acceleration while staying very flexible thanks to userspace network programming. SmartNICs (aka DPUs) are used for bare metal deployments to isolate different customers.","title":"Network"},{"location":"#storage","text":"Most applications need state. Gardener on Metal provides state via Block Storage and an S3-compatible Object Storage. We have forked the Rook-Ceph-Operator and adapted it to fit our production requirements. Most modifications are about day-2 operations of Ceph clusters. The target is to automate as much as possible of Ceph operations. Storage is hard. We want to make easier and available for everyone. Gardener on Metal in a first step does not adopt the hyperconverged infrastructure philosophy. We build clusters with dedicated compute and storage nodes. This allows us to scale both parts independent from another. Also storage is not always the same. You may want to change the ratio of NVMe space to HDD space. This allows to optimize according to your workload. If you need more HDD space for your datalake or your backups, just add more servers with spinning disks. Gardener on Metal's Ceph operator cares about integrating the new storage nodes into the cluster.","title":"Storage"},{"location":"#compute","text":"Compute comes in different flavors with Gardener on Metal: As a bare metal instance or as a virtual machine. A bare metal instance is also often called a dedicated server or root server . The customer has full access to the server's hardware. We use SmartNICs aka DPUs to isolate a physical server from other customers. Using a DPU the bare metal instance can consume additional managed services like managed Firewall, NAT gateways or network block devices. A Virtual Machine is in the end also just a Linux process . We deploy virtual machines via Kubernetes to a set of servers that act as hypervisors.The customer gets root access to the VM and can do whatever he wants with the VM. Kubernetes comes as a Service with Gardener on Metal. The customer does not need to care about bare metal or VM administrations when a Gardener managed Kubernetes cluster is used. Gardener will care about installing all required packages and Kubernetes artifacts. With the provided kubeconfig customers can connect to their Kubernetes clusters within minutes.","title":"Compute"},{"location":"storage/","text":"Storage \u00b6 Gardener on Metal is really good in regards to storage - we use Ceph! For lower operational costs we have even created our own Kubernetes Ceph operator.","title":"Storage"},{"location":"storage/#storage","text":"Gardener on Metal is really good in regards to storage - we use Ceph! For lower operational costs we have even created our own Kubernetes Ceph operator.","title":"Storage"},{"location":"api/api/","text":"On Metal API \u00b6 Something about API goes here ...","title":"On Metal API"},{"location":"api/api/#on-metal-api","text":"Something about API goes here ...","title":"On Metal API"},{"location":"compute/idea/","text":"Idea \u00b6 Gardener on Metal is really good in regards to compute!","title":"Idea"},{"location":"compute/idea/#idea","text":"Gardener on Metal is really good in regards to compute!","title":"Idea"},{"location":"compute/onboarding/","text":"Hardware onboarding \u00b6 Gardener on Metal has the coolest hardware onboarding!","title":"Hardware onboarding"},{"location":"compute/onboarding/#hardware-onboarding","text":"Gardener on Metal has the coolest hardware onboarding!","title":"Hardware onboarding"},{"location":"compute/oob/","text":"OOB Management \u00b6 Gardener on Metal has the coolest OOB management!","title":"OOB Management"},{"location":"compute/oob/#oob-management","text":"Gardener on Metal has the coolest OOB management!","title":"OOB Management"},{"location":"compute/virtualization/","text":"Virtualization \u00b6 Virtualization is cool.","title":"Overview"},{"location":"compute/virtualization/#virtualization","text":"Virtualization is cool.","title":"Virtualization"},{"location":"manual/intro/","text":"Introduction \u00b6 This Operations Manual describes how to install Gardener on Metal on a new infrastructure.","title":"Introduction"},{"location":"manual/intro/#introduction","text":"This Operations Manual describes how to install Gardener on Metal on a new infrastructure.","title":"Introduction"},{"location":"manual/harbor/chart_museum/","text":"Using helm to upload charts to Harbor Chart Museum \u00b6 Add push plugin to helm helm plugin install https://github.com/chartmuseum/helm-push Add harbor to list of repos helm repo add --username = USERNAME --password = PASSWORD harbor https://core.harbor.onmetal.de/chartrepo/library Push your local chart to museum helm push --username = USERNAME --password = PASSWORD path/to/chart_name harbor Update remote chars helm repo update Install chart from harbor chart museum helm install release_name harbor/chart_name","title":"Installation"},{"location":"manual/harbor/chart_museum/#using-helm-to-upload-charts-to-harbor-chart-museum","text":"Add push plugin to helm helm plugin install https://github.com/chartmuseum/helm-push Add harbor to list of repos helm repo add --username = USERNAME --password = PASSWORD harbor https://core.harbor.onmetal.de/chartrepo/library Push your local chart to museum helm push --username = USERNAME --password = PASSWORD path/to/chart_name harbor Update remote chars helm repo update Install chart from harbor chart museum helm install release_name harbor/chart_name","title":"Using helm to upload charts to Harbor Chart Museum"},{"location":"manual/harbor/docker_registry/","text":"Uploading docker image to Harbor Registry \u00b6 Login to repo docker login core.harbor.onmetal.de Tag your image docker tag local-server/app_path/app_name:version core.harbor.onmetal.de/app_path/app_name:version Push your image docker push core.harbor.onmetal.de/app_path/app_name:version (Optionally) Logout docker logout core.harbor.onmetal.de","title":"Usage"},{"location":"manual/harbor/docker_registry/#uploading-docker-image-to-harbor-registry","text":"Login to repo docker login core.harbor.onmetal.de Tag your image docker tag local-server/app_path/app_name:version core.harbor.onmetal.de/app_path/app_name:version Push your image docker push core.harbor.onmetal.de/app_path/app_name:version (Optionally) Logout docker logout core.harbor.onmetal.de","title":"Uploading docker image to Harbor Registry"},{"location":"manual/virtualization/usage/","text":"How to deploy ? \u00b6 This document walks through how to deploy the virtualization stack on a baremetals using virt-on-metal as of now. Please note, this document only represents the current state and work is in progress yet. Prerequisites \u00b6 4 baremetal machines with internal connectivity. While this document is tested for debian based hosts, it should be equally applicable for other linux-distributions with minor adaptations. We'll refer them as compute-1, compute-2, compute-3 and compute-4. One Baremetal will be a dedicated NAT Gateway machine, one as a kubernetes control-plane and two as a kubernetes nodes. VMs will only be deployed on the two kubernetes nodes. At a glance \u00b6 Virt-on-metal is a system to manage virtual machines at scale in a cloud-native fashion. It uses kubernetes premitives such as CRDS for VM management, libvirt as a base virtualization technology, MPLS tunnels for networking, and DHCP for IP assignment and iPXE boot. VMs are currently boot-up with PXE mechanism, and it plans to also support disk-based boot-up. Steps \u00b6 Install Kubernetes using kubeadm \u00b6 Disable swap and install kubeadm . This will be the kubernetes without kube-proxy using cilium plugin, which is capable of enabling network management in a more efficient way. Swapoff -a kubeadm init --skip-phases = addon/kube-proxy Once cluster is up and running, install cilium-plugin using helm. # Install Helm curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh # Install Cilium helm repo add cilium https://helm.cilium.io/ helm --namespace kube-system install cilium cilium/cilium --version v1.10.2 --set cni.binPath = /usr/lib/cni cp usr/lib/cni/cilium-cni /opt/cni/bin/cilium-cni # this depends on where kubelet looks for cni binary. Update Cilium CNI daemonset with following 2 changes. At cilium-agent command, add following extra args: --mtu = 1500 --k8s-api-server = https://<KUBE_APISERVER_IP>:6443 Taint compute-4 as \"func=NAT\". kubectl taint nodes compute-4 func = nat:NoSchedule Install libvirt daemon \u00b6 Install libvirt on compute-2, and compute-3. Compute-2 and Compute-3 will be the kubernetes nodes, where Virtual Machines will be spawned. sudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils libguestfs-tools genisoimage virtinst libosinfo-bin Deploy VMlet controller \u00b6 Deploy the kubeconfig secret and daemonset in the kubernetes cluster. kubectl apply -f https://github.com/onmetal/vmlet/blob/main/config/manager/kubecfg-secret.yaml # Replace the kubeconfig with actual kubeconfig. kubectl apply -f https://github.com/onmetal/vmlet/blob/main/config/manager/manager-ds.yaml Deploy NAT controller \u00b6 The NAT controller is responsible of creating NAT on the server being tainted with \"func=NAT\". It is mainly responsable of using network namespace to create snat for VMs with the same virtual network ID. Additonally, it initialises MPLS tunnels with routers so that packets that are natted can be sent to routers. kubectl apply -f https://github.com/onmetal/virtualisation-benchmark/blob/main/mpls-nat-operator.yaml After executing the above command, a network namespace named nat-100, a VRF named vrf-100 will be created. Corresponding default and MPLS routing rules will be inserted automatically. Deploy DHCP Server \u00b6 The configuration of private DHCP server is not automated. A dedicated VM needs to be spinned up to host the Kea DHCP server on the node serving as NAT. At this stage, it is reconmmended to use a prebuilt qcow2 image to avoid installation of kea and writing dhcp configuration files. This VM has to be manually attached to VRF that was created in the previous step, and routing rule pointing to this VM has to be added. Assuming this VM uses the IP address, 10.1.1.2, and its tap has the name, tap111996580, the following commands shall be executed: ip link set tap111996580 master vrf-100 ip route add 10 .1.1.2/32 dev tap111996580 vrf vrf-100 Inside this DHCP VM, assuming the prebuilt qcow2 is used to boot it up, the configuration file containing the subnet and VMs' IP address information has to be adapted. If the Kea docker image does not exist, the Dockerfile can be used to build a local image. docker build -t kea-dhcp4:1.9.8 --network = host . docker run -v [ -d ] /root/kea-docker/kea-dhcp4-conf:/etc/kea --network host --name kea-dhcp4 kea-dhcp4:1.9.8 Deploy VNet-controller \u00b6 VNet-controller is deployed on each worker machine that hosts VMs. Its main task is to provide a configured TAP for VM, and corresponding routing rules. It is also responsable for starting isc-dhcp-relay to support iPXE booting. k apply -f https://github.com/onmetal/virtualisation-benchmark/blob/main/mpls-tun-operator.yaml Deploy VM-scheduler \u00b6 Deploy vm-scheduler deployment. k apply -f https://github.com/onmetal/vm-scheduler/blob/main/config/manager/manager.yaml Create First Virtual Machine \u00b6 Use following configuration to create the first virtual machine. @TODO Hardik Tao, this depends on how we configure the IP range in DHCP server etc. kubectl apply -f https://github.com/onmetal/vmlet/blob/main/config/samples/vm7.yaml","title":"Installation"},{"location":"manual/virtualization/usage/#how-to-deploy","text":"This document walks through how to deploy the virtualization stack on a baremetals using virt-on-metal as of now. Please note, this document only represents the current state and work is in progress yet.","title":"How to deploy ?"},{"location":"manual/virtualization/usage/#prerequisites","text":"4 baremetal machines with internal connectivity. While this document is tested for debian based hosts, it should be equally applicable for other linux-distributions with minor adaptations. We'll refer them as compute-1, compute-2, compute-3 and compute-4. One Baremetal will be a dedicated NAT Gateway machine, one as a kubernetes control-plane and two as a kubernetes nodes. VMs will only be deployed on the two kubernetes nodes.","title":"Prerequisites"},{"location":"manual/virtualization/usage/#at-a-glance","text":"Virt-on-metal is a system to manage virtual machines at scale in a cloud-native fashion. It uses kubernetes premitives such as CRDS for VM management, libvirt as a base virtualization technology, MPLS tunnels for networking, and DHCP for IP assignment and iPXE boot. VMs are currently boot-up with PXE mechanism, and it plans to also support disk-based boot-up.","title":"At a glance"},{"location":"manual/virtualization/usage/#steps","text":"","title":"Steps"},{"location":"manual/virtualization/usage/#install-kubernetes-using-kubeadm","text":"Disable swap and install kubeadm . This will be the kubernetes without kube-proxy using cilium plugin, which is capable of enabling network management in a more efficient way. Swapoff -a kubeadm init --skip-phases = addon/kube-proxy Once cluster is up and running, install cilium-plugin using helm. # Install Helm curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh # Install Cilium helm repo add cilium https://helm.cilium.io/ helm --namespace kube-system install cilium cilium/cilium --version v1.10.2 --set cni.binPath = /usr/lib/cni cp usr/lib/cni/cilium-cni /opt/cni/bin/cilium-cni # this depends on where kubelet looks for cni binary. Update Cilium CNI daemonset with following 2 changes. At cilium-agent command, add following extra args: --mtu = 1500 --k8s-api-server = https://<KUBE_APISERVER_IP>:6443 Taint compute-4 as \"func=NAT\". kubectl taint nodes compute-4 func = nat:NoSchedule","title":"Install Kubernetes using kubeadm"},{"location":"manual/virtualization/usage/#install-libvirt-daemon","text":"Install libvirt on compute-2, and compute-3. Compute-2 and Compute-3 will be the kubernetes nodes, where Virtual Machines will be spawned. sudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils libguestfs-tools genisoimage virtinst libosinfo-bin","title":"Install libvirt daemon"},{"location":"manual/virtualization/usage/#deploy-vmlet-controller","text":"Deploy the kubeconfig secret and daemonset in the kubernetes cluster. kubectl apply -f https://github.com/onmetal/vmlet/blob/main/config/manager/kubecfg-secret.yaml # Replace the kubeconfig with actual kubeconfig. kubectl apply -f https://github.com/onmetal/vmlet/blob/main/config/manager/manager-ds.yaml","title":"Deploy VMlet controller"},{"location":"manual/virtualization/usage/#deploy-nat-controller","text":"The NAT controller is responsible of creating NAT on the server being tainted with \"func=NAT\". It is mainly responsable of using network namespace to create snat for VMs with the same virtual network ID. Additonally, it initialises MPLS tunnels with routers so that packets that are natted can be sent to routers. kubectl apply -f https://github.com/onmetal/virtualisation-benchmark/blob/main/mpls-nat-operator.yaml After executing the above command, a network namespace named nat-100, a VRF named vrf-100 will be created. Corresponding default and MPLS routing rules will be inserted automatically.","title":"Deploy NAT controller"},{"location":"manual/virtualization/usage/#deploy-dhcp-server","text":"The configuration of private DHCP server is not automated. A dedicated VM needs to be spinned up to host the Kea DHCP server on the node serving as NAT. At this stage, it is reconmmended to use a prebuilt qcow2 image to avoid installation of kea and writing dhcp configuration files. This VM has to be manually attached to VRF that was created in the previous step, and routing rule pointing to this VM has to be added. Assuming this VM uses the IP address, 10.1.1.2, and its tap has the name, tap111996580, the following commands shall be executed: ip link set tap111996580 master vrf-100 ip route add 10 .1.1.2/32 dev tap111996580 vrf vrf-100 Inside this DHCP VM, assuming the prebuilt qcow2 is used to boot it up, the configuration file containing the subnet and VMs' IP address information has to be adapted. If the Kea docker image does not exist, the Dockerfile can be used to build a local image. docker build -t kea-dhcp4:1.9.8 --network = host . docker run -v [ -d ] /root/kea-docker/kea-dhcp4-conf:/etc/kea --network host --name kea-dhcp4 kea-dhcp4:1.9.8","title":"Deploy DHCP Server"},{"location":"manual/virtualization/usage/#deploy-vnet-controller","text":"VNet-controller is deployed on each worker machine that hosts VMs. Its main task is to provide a configured TAP for VM, and corresponding routing rules. It is also responsable for starting isc-dhcp-relay to support iPXE booting. k apply -f https://github.com/onmetal/virtualisation-benchmark/blob/main/mpls-tun-operator.yaml","title":"Deploy VNet-controller"},{"location":"manual/virtualization/usage/#deploy-vm-scheduler","text":"Deploy vm-scheduler deployment. k apply -f https://github.com/onmetal/vm-scheduler/blob/main/config/manager/manager.yaml","title":"Deploy VM-scheduler"},{"location":"manual/virtualization/usage/#create-first-virtual-machine","text":"Use following configuration to create the first virtual machine. @TODO Hardik Tao, this depends on how we configure the IP range in DHCP server etc. kubectl apply -f https://github.com/onmetal/vmlet/blob/main/config/samples/vm7.yaml","title":"Create First Virtual Machine"},{"location":"network/address_structure/","text":"IPv6 Address Structure \u00b6 A /50 IPv6 prefix will be assigned to every partition cluster (e.g. 2001:db8:123:4000::/50 ). Every node that is part of this cluster will get a /64 . The lowest /64 will be used as Kubernetes Service CIDR (here: 2001:db8:123:4000::/64 ). As a result the maximum number of servers a cluster can hold is 0x3fff = 16383 . The Kubernetes Service address range must not be reachable from outside of the partition cluster. Nodes of a partition cluster can be hypervisor machines, that run in the underlay network or SmartNICs. Every node will get a /64 IPv6 prefix assigned (e.g. 2001:db8:123:4001::/64 ). The node's loopback device is configured with the first IP out of this range: 2001:db8:123:4001::/128 . The first 32 bits of the node's address range is used for SRv6 SIDs, using the range 0x1 - 0xffffffff (here: 2001:db8:123:4001:0:1::/96 - 2001:db8:123:4001:ffff:ffff::/96 ). Those prefixes will be used to route SRv6 encapsulated traffic to the VMs. VNI 0 is used for the Kernel itself. In our example 2001:db8:123:4001:0:0::/96 would be routed to the Kernel. The pod cidr is part of this range: 2001:db8:123:4001:0:0:1::/112 and allows to run 65536 pods per node ( 2001:db8:123:4001:0:0:1:0 - 2001:db8:123:4001:0:0:1:ffff ).","title":"Address Structure"},{"location":"network/address_structure/#ipv6-address-structure","text":"A /50 IPv6 prefix will be assigned to every partition cluster (e.g. 2001:db8:123:4000::/50 ). Every node that is part of this cluster will get a /64 . The lowest /64 will be used as Kubernetes Service CIDR (here: 2001:db8:123:4000::/64 ). As a result the maximum number of servers a cluster can hold is 0x3fff = 16383 . The Kubernetes Service address range must not be reachable from outside of the partition cluster. Nodes of a partition cluster can be hypervisor machines, that run in the underlay network or SmartNICs. Every node will get a /64 IPv6 prefix assigned (e.g. 2001:db8:123:4001::/64 ). The node's loopback device is configured with the first IP out of this range: 2001:db8:123:4001::/128 . The first 32 bits of the node's address range is used for SRv6 SIDs, using the range 0x1 - 0xffffffff (here: 2001:db8:123:4001:0:1::/96 - 2001:db8:123:4001:ffff:ffff::/96 ). Those prefixes will be used to route SRv6 encapsulated traffic to the VMs. VNI 0 is used for the Kernel itself. In our example 2001:db8:123:4001:0:0::/96 would be routed to the Kernel. The pod cidr is part of this range: 2001:db8:123:4001:0:0:1::/112 and allows to run 65536 pods per node ( 2001:db8:123:4001:0:0:1:0 - 2001:db8:123:4001:0:0:1:ffff ).","title":"IPv6 Address Structure"},{"location":"network/controlplane/","text":"Control Plane \u00b6 Requirements \u00b6 If a node loses connection, the routes must be removed from the system. A simple Kafka Queue does not do this. BGP removes all routes from the routing table, when a peer, that the routes were received from, disconnects.","title":"Control Plane"},{"location":"network/controlplane/#control-plane","text":"","title":"Control Plane"},{"location":"network/controlplane/#requirements","text":"If a node loses connection, the routes must be removed from the system. A simple Kafka Queue does not do this. BGP removes all routes from the routing table, when a peer, that the routes were received from, disconnects.","title":"Requirements"},{"location":"network/ddos/","text":"DDoS Protection \u00b6 When running commercial services on your infrastructure a good DDoS protection saves you a lot of money - or you spend a lot of money, as commercial DDoS protection solutions come at a high price. We do not want to go into the cost details here. Instead we want to provide an overview over the different types of DDoS protection and how to implement them. Blackholing \u00b6 The simplest type of DDoS protection is blackholing. It is specified in RFC7999 . Basically blackholing means shutting down single IP addresses. Traffic that usually gets transported to the service by an IP transit provider or via an internet exchange will be dropped. E.g. you announce the IP range 192.0.2.0/24 and a service under the IP address 192.0.2.23 is being DDoSed. To protect your infrastructure you ask your IP transit provider to drop all traffic to the prefix 192.0.2.23/32 . This is done by announcing this prefix with the BGP community 666 to your IP transit provider. Hurricane Electric and DE-CIX explain how to do blackholing on their websites. As a result you will not get any packets to 192.0.2.23 anymore. This means this specific service is down - but the rest of your infrastructure still works. Blackholing does not protect the attacked service! Instead it protects the rest of your infrastructure. It sacrifices the attacked service for the greater good. FlowSpec \u00b6 FlowSpec is a bit more advanced to blackholing. In principle it works similar to blackholing: You ask your transit provider to drop traffic. But with FlowSpec you can implement a fine grained filter what traffic to drop. FlowSpec is described in RFC5575 . With FlowSpec filter based on the following parameters: 1. Destination Prefix 2. Source Prefix 3. IP Protocol (matching the IP protocol byte field in the IP packet) 4. Port (matches source OR destination port) 5. Destination Port 6. Source Port (e.g. by filtering for source port 53 you can filter DNS amplification attacks) 7. ICMP type 8. ICMP code 9. TCP flags 10. Packet length 11. DSCP field 12. Fragment (don't fragment, is a fragment, first fragment, last fragment) If you can identify the nature of the DDoS attack and you are able to match the attack with the options given by FlowSpec you can let your transit provider drop all requests of the attack. Ideally your service would stay online. Obviously creating a pattern set, that matches all of the attack's packets is not very easy. So you may think about outsourcing DDoS protection to a scrubbing center - they have sophisticated machine learning models for that. Scrubbing Centers \u00b6 You can pipe all your incoming traffic through a scrubbing center to clean it. The scrubbing center will announce your IP prefixes to the rest of the world using BGP. They will analyse the traffic based on layer 2-4 and try to clean it. The cleaned traffic will be sent to you - usually via a GRE tunnel or a PNI. A scrubbing center is no replacement for an IP transit provider. They will only forward incoming traffic. For outgoing traffic you still need a transit provider. There are usually two deployment models of scrubbing centers: always-on and on-demand . In the always-on case your traffic will always go through the scrubbing center (this is expensive). In the on-demand case you will only pipe your incoming traffic through the scrubbing center, when you are under attack (no illusions! this is also expensive!). Usually you pay scrubbing centers based on the provisioned bandwidth of cleaned traffic. Additionally there will be fees based on the number of prefixes and GRE tunnels or PNIs and for 24x7 support. Here are a few scrubbing services in alphabetical order: Akamai Prolexic Arbor Cloudflare Magic Transit Radware Layer 7 Cloud Firewalls / CDNs \u00b6 You can also let your application care about DDoS protection. Maybe they need a CDN anyways? Then a valid approach could also be to just say: \"Hey, all applications need to use a Cloud CDN or WAF. Every service that gets DDoSed will otherwise be blackholed!\" - If you are a small service provider this is probably the best strategy to get started. Implementing a Layer 7 CDN/WAF is quite easy and you do not have to talk to any sales person. Layer 7 CDN / WAF combinations also come with a few other niceties. They do TLS offloading near the edge, accelerate static content delivery using caches or can detect and filter out SQL injections. Be careful, Layer 7 services can be even more expensive than scrubbing centers - but usually the costs scale pretty nice with your services' traffic! Here are a few Layer 7 CDN/WAF services in alphabetical order: Akamai CDN AWS Cloudfront , Shield , WAF Cloudflare GCP CDN , Cloud Armor","title":"DDoS Protection"},{"location":"network/ddos/#ddos-protection","text":"When running commercial services on your infrastructure a good DDoS protection saves you a lot of money - or you spend a lot of money, as commercial DDoS protection solutions come at a high price. We do not want to go into the cost details here. Instead we want to provide an overview over the different types of DDoS protection and how to implement them.","title":"DDoS Protection"},{"location":"network/ddos/#blackholing","text":"The simplest type of DDoS protection is blackholing. It is specified in RFC7999 . Basically blackholing means shutting down single IP addresses. Traffic that usually gets transported to the service by an IP transit provider or via an internet exchange will be dropped. E.g. you announce the IP range 192.0.2.0/24 and a service under the IP address 192.0.2.23 is being DDoSed. To protect your infrastructure you ask your IP transit provider to drop all traffic to the prefix 192.0.2.23/32 . This is done by announcing this prefix with the BGP community 666 to your IP transit provider. Hurricane Electric and DE-CIX explain how to do blackholing on their websites. As a result you will not get any packets to 192.0.2.23 anymore. This means this specific service is down - but the rest of your infrastructure still works. Blackholing does not protect the attacked service! Instead it protects the rest of your infrastructure. It sacrifices the attacked service for the greater good.","title":"Blackholing"},{"location":"network/ddos/#flowspec","text":"FlowSpec is a bit more advanced to blackholing. In principle it works similar to blackholing: You ask your transit provider to drop traffic. But with FlowSpec you can implement a fine grained filter what traffic to drop. FlowSpec is described in RFC5575 . With FlowSpec filter based on the following parameters: 1. Destination Prefix 2. Source Prefix 3. IP Protocol (matching the IP protocol byte field in the IP packet) 4. Port (matches source OR destination port) 5. Destination Port 6. Source Port (e.g. by filtering for source port 53 you can filter DNS amplification attacks) 7. ICMP type 8. ICMP code 9. TCP flags 10. Packet length 11. DSCP field 12. Fragment (don't fragment, is a fragment, first fragment, last fragment) If you can identify the nature of the DDoS attack and you are able to match the attack with the options given by FlowSpec you can let your transit provider drop all requests of the attack. Ideally your service would stay online. Obviously creating a pattern set, that matches all of the attack's packets is not very easy. So you may think about outsourcing DDoS protection to a scrubbing center - they have sophisticated machine learning models for that.","title":"FlowSpec"},{"location":"network/ddos/#scrubbing-centers","text":"You can pipe all your incoming traffic through a scrubbing center to clean it. The scrubbing center will announce your IP prefixes to the rest of the world using BGP. They will analyse the traffic based on layer 2-4 and try to clean it. The cleaned traffic will be sent to you - usually via a GRE tunnel or a PNI. A scrubbing center is no replacement for an IP transit provider. They will only forward incoming traffic. For outgoing traffic you still need a transit provider. There are usually two deployment models of scrubbing centers: always-on and on-demand . In the always-on case your traffic will always go through the scrubbing center (this is expensive). In the on-demand case you will only pipe your incoming traffic through the scrubbing center, when you are under attack (no illusions! this is also expensive!). Usually you pay scrubbing centers based on the provisioned bandwidth of cleaned traffic. Additionally there will be fees based on the number of prefixes and GRE tunnels or PNIs and for 24x7 support. Here are a few scrubbing services in alphabetical order: Akamai Prolexic Arbor Cloudflare Magic Transit Radware","title":"Scrubbing Centers"},{"location":"network/ddos/#layer-7-cloud-firewalls-cdns","text":"You can also let your application care about DDoS protection. Maybe they need a CDN anyways? Then a valid approach could also be to just say: \"Hey, all applications need to use a Cloud CDN or WAF. Every service that gets DDoSed will otherwise be blackholed!\" - If you are a small service provider this is probably the best strategy to get started. Implementing a Layer 7 CDN/WAF is quite easy and you do not have to talk to any sales person. Layer 7 CDN / WAF combinations also come with a few other niceties. They do TLS offloading near the edge, accelerate static content delivery using caches or can detect and filter out SQL injections. Be careful, Layer 7 services can be even more expensive than scrubbing centers - but usually the costs scale pretty nice with your services' traffic! Here are a few Layer 7 CDN/WAF services in alphabetical order: Akamai CDN AWS Cloudfront , Shield , WAF Cloudflare GCP CDN , Cloud Armor","title":"Layer 7 Cloud Firewalls / CDNs"},{"location":"network/encapsulation/","text":"Encapsulation \u00b6 Geneve \u00b6 Geneve, the Generic Network Virtualization Encapsulation is defined in RFC8926 . 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 Outer Ethernet Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Destination MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Destination MAC Address | Outer Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Optional Ethertype=C-Tag 802.1Q| Outer VLAN Tag Information | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Ethertype = 0x86DD IPv6 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Outer IPv6 Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| Traffic Class | Flow Label | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Payload Length | NxtHdr=17 UDP | Hop Limit | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Outer Source IPv6 Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Outer Destination IPv6 Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Outer UDP Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Port = xxxx | Dest Port = 6081 Geneve | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | UDP Length | UDP Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Geneve Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Ver| Opt Len |O|C| Rsvd. | ProtocolType 0x0800 or 0x0806 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Virtual Network Identifier (VNI) | Reserved | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | ~ Variable-Length Options ~ | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Inner IP Header (example payload): +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Destination MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Destination MAC Address | Inner Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Optional Ethertype=C-Tag 802.1Q| Inner VLAN Tag Information | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Payload: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Identification |Flags| Fragment Offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Time to Live | Protocol | Header Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Source IPv4 Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Destination IPv4 Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | | | IP Payload | ... | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Frame Check Sequence: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | New Frame Check Sequence (FCS) for Outer Ethernet Frame | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ SRv6 \u00b6 The IPv6 Segment Routing Header (SRH) is defined in RFC8754 . You can find more information about SRv6 at https://www.segment-routing.net and in this slide deck . An SRv6 encapsulated IPv4 packet typically looks like this: 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 Outer Ethernet Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Destination MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Destination MAC Address | Outer Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Optional Ethertype=C-Tag 802.1Q| Outer VLAN Tag Information | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Ethertype = 0x86DD IPv6 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Outer IPv6 Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| Traffic Class | Flow Label | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Payload Length | NxtHdr = 43 | Hop Limit | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Outer Source IPv6 Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Outer Destination IPv6 Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Segment Routing Header (SRH): +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Next Header | Hdr Ext Len | Routing Type | Segments Left | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Last Entry | Flags | Tag | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | Segment List[0] (128-bit IPv6 address) | | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | | ... | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | Segment List[n] (128-bit IPv6 address) | | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ // // // Optional Type Length Value objects (variable) // // // +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Payload: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Identification |Flags| Fragment Offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Time to Live | Protocol | Header Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Source IPv4 Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Destination IPv4 Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | | | IP Payload | ... | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Frame Check Sequence: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | New Frame Check Sequence (FCS) for Outer Ethernet Frame | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+","title":"Encapsulation"},{"location":"network/encapsulation/#encapsulation","text":"","title":"Encapsulation"},{"location":"network/encapsulation/#geneve","text":"Geneve, the Generic Network Virtualization Encapsulation is defined in RFC8926 . 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 Outer Ethernet Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Destination MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Destination MAC Address | Outer Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Optional Ethertype=C-Tag 802.1Q| Outer VLAN Tag Information | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Ethertype = 0x86DD IPv6 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Outer IPv6 Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| Traffic Class | Flow Label | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Payload Length | NxtHdr=17 UDP | Hop Limit | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Outer Source IPv6 Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Outer Destination IPv6 Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Outer UDP Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Port = xxxx | Dest Port = 6081 Geneve | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | UDP Length | UDP Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Geneve Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Ver| Opt Len |O|C| Rsvd. | ProtocolType 0x0800 or 0x0806 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Virtual Network Identifier (VNI) | Reserved | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | ~ Variable-Length Options ~ | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Inner IP Header (example payload): +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Destination MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Destination MAC Address | Inner Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Optional Ethertype=C-Tag 802.1Q| Inner VLAN Tag Information | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Payload: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Identification |Flags| Fragment Offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Time to Live | Protocol | Header Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Source IPv4 Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Destination IPv4 Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | | | IP Payload | ... | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Frame Check Sequence: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | New Frame Check Sequence (FCS) for Outer Ethernet Frame | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+","title":"Geneve"},{"location":"network/encapsulation/#srv6","text":"The IPv6 Segment Routing Header (SRH) is defined in RFC8754 . You can find more information about SRv6 at https://www.segment-routing.net and in this slide deck . An SRv6 encapsulated IPv4 packet typically looks like this: 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 Outer Ethernet Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Destination MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Destination MAC Address | Outer Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Outer Source MAC Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Optional Ethertype=C-Tag 802.1Q| Outer VLAN Tag Information | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Ethertype = 0x86DD IPv6 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Outer IPv6 Header: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| Traffic Class | Flow Label | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Payload Length | NxtHdr = 43 | Hop Limit | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Outer Source IPv6 Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Outer Destination IPv6 Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Segment Routing Header (SRH): +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Next Header | Hdr Ext Len | Routing Type | Segments Left | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Last Entry | Flags | Tag | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | Segment List[0] (128-bit IPv6 address) | | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | | ... | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | Segment List[n] (128-bit IPv6 address) | | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ // // // Optional Type Length Value objects (variable) // // // +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Payload: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Identification |Flags| Fragment Offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Time to Live | Protocol | Header Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Source IPv4 Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Inner Destination IPv4 Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | | | IP Payload | ... | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Frame Check Sequence: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | New Frame Check Sequence (FCS) for Outer Ethernet Frame | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+","title":"SRv6"},{"location":"network/idea/","text":"Idea \u00b6 Gardener on Metal provides an infrastructure to run Cloud Native Applications. Multiple tenants should use this infrastructure in a secure way. They should not interfere with each others' workload. Noisy neighbor disturbances should be avoided as best as possible. Clos Topology \u00b6 Gardener on Metal relies on a Layer 3 based clos topology. Layer 2 is only used between to connected nodes. All traffic that travels across the network will be routed. BGP is used to distribute routing information. More information about Layer 3 clos topologies in data centers and BGP can be found in RFC7938 . The servers are connected to two different Leaf switches. 2x25 Gbps, 2x40 Gbps, 2x100 Gbps are common bandwidths. Leaf and Spine switches run SONiC and do IP routing. A Gardener on Metal environment can host multiple customers. It is a multi-tenant environment. Customers can be evil and may not be trusted! Therefore, tenants\u2019 workload must be separated from each other and from the underlying infrastructure. Gardener on Metal is able to provide two server types to the customers: Virtual Machines (VMs) using KVM and bare-metal servers. In the VM case the hypervisor is sandboxing the customer\u2019s machine. The hypervisor is under control of the infrastructure provider (onmetal Team) and can be trusted. In the bare-metal case the bare-metal server cannot be trusted, as it will be under customer\u2019s control. Gardener on Metal uses a DPU, that is not controlled by its host system but from the outside, to sandbox the bare-metal server. In this case the DPU must not provide access to the underlay network to the bare-metal server but only access to the tenants overlay network. From a networking and storage perspective the bare-metal server should feel like a virtual machine and have the same functionality (firewalling, network block storage etc.). The DPU should provide a NIC to the bare-metal server via PCIe, to a VM via SR-IOV as a Virtual Function or using a virtio-net device. IPv4 & IPv6 \u00b6 Most of enterprise applications that exist use IPv4 networking. But IPv4 addresses are getting scarce and some ISPs do not provide dedicated IPv4 addresses to their clients anymore. IPv6 is newer and the better internet protocol. Gardener on Metal customers should be able to chose if they want to run their cluster with an IPv4-only network, IPv4/IPv6 dual-stack or IPv6-only. The large Cloud Service Providers retro-fit their networks with IPv6 support. That's a tedious task and if you build an infrastructure with the pure focus on IPv4 migrating to IPv6 will be very difficult. With Gardener on Metal IPv6 is a first class citizen. Layer 3 unicast only \u00b6 Modern applications use IP based communication. In the early days of networking there were also other protocols like IPX or AppleTalk. Thankfully those days are over. Still, we see some applications, that use features from the Ethernet layer, that most IP networks are based on. A typical use case is cache invalidation, that a Java application triggers via UDP broadcasts to its neighbors (see Java Caching System ). In datacenter networking and especially virtual networking so called BUM traffic creates a lot of pain for the operator. Also modern queueing and pubsub solutions came up: ActiveMQ, RabbitMQ, Kafka, NATS. Distributing information using such queueing system is much more comfortable and reliable. We think Ethernet based networking does not fit into the Cloud Native world! You should use IP-based protocols instead! By removing Ethernet networks from Gardener on Metal we save lots of complexity in our infrastructure. All networking is IP based and every packet will be routed. All traffic coming from a VM or a bare metal machine will be routed by the hypervisor or a SmartNIC to the target. There is no Ethernet connectivity between two machines but an IP network. A VM has its hypervisor configured as a router: # ip route show default via 169.254.0.1 dev eno0 proto dhcp src 233.252.0.103 metric 1024 169.254.0.1/32 dev eno0 proto dhcp scope link src 233.252.0.103 metric 1024 # ip -6 route show default via fe80::1 dev eno0 proto dhcpv6 src 2001:db8::1 metric 256 pref medium fe80::/64 dev eno0 proto kernel metric 256 pref medium We reduce complexity and also increase resiliency a lot by avoiding BUM traffic. IPv6 underlay \u00b6 The Gardener on Metal underlay network uses IPv6 only. We have no need to support IPv4 in the underlay. All customer traffic is encapsulated in IPv6 and sent through our underlay network. IPv6 helps structuring the address space and uses globally unique addresses. No need to use the same IPv4 address space multiple times and trying to find unused address ranges.","title":"Idea"},{"location":"network/idea/#idea","text":"Gardener on Metal provides an infrastructure to run Cloud Native Applications. Multiple tenants should use this infrastructure in a secure way. They should not interfere with each others' workload. Noisy neighbor disturbances should be avoided as best as possible.","title":"Idea"},{"location":"network/idea/#clos-topology","text":"Gardener on Metal relies on a Layer 3 based clos topology. Layer 2 is only used between to connected nodes. All traffic that travels across the network will be routed. BGP is used to distribute routing information. More information about Layer 3 clos topologies in data centers and BGP can be found in RFC7938 . The servers are connected to two different Leaf switches. 2x25 Gbps, 2x40 Gbps, 2x100 Gbps are common bandwidths. Leaf and Spine switches run SONiC and do IP routing. A Gardener on Metal environment can host multiple customers. It is a multi-tenant environment. Customers can be evil and may not be trusted! Therefore, tenants\u2019 workload must be separated from each other and from the underlying infrastructure. Gardener on Metal is able to provide two server types to the customers: Virtual Machines (VMs) using KVM and bare-metal servers. In the VM case the hypervisor is sandboxing the customer\u2019s machine. The hypervisor is under control of the infrastructure provider (onmetal Team) and can be trusted. In the bare-metal case the bare-metal server cannot be trusted, as it will be under customer\u2019s control. Gardener on Metal uses a DPU, that is not controlled by its host system but from the outside, to sandbox the bare-metal server. In this case the DPU must not provide access to the underlay network to the bare-metal server but only access to the tenants overlay network. From a networking and storage perspective the bare-metal server should feel like a virtual machine and have the same functionality (firewalling, network block storage etc.). The DPU should provide a NIC to the bare-metal server via PCIe, to a VM via SR-IOV as a Virtual Function or using a virtio-net device.","title":"Clos Topology"},{"location":"network/idea/#ipv4-ipv6","text":"Most of enterprise applications that exist use IPv4 networking. But IPv4 addresses are getting scarce and some ISPs do not provide dedicated IPv4 addresses to their clients anymore. IPv6 is newer and the better internet protocol. Gardener on Metal customers should be able to chose if they want to run their cluster with an IPv4-only network, IPv4/IPv6 dual-stack or IPv6-only. The large Cloud Service Providers retro-fit their networks with IPv6 support. That's a tedious task and if you build an infrastructure with the pure focus on IPv4 migrating to IPv6 will be very difficult. With Gardener on Metal IPv6 is a first class citizen.","title":"IPv4 &amp; IPv6"},{"location":"network/idea/#layer-3-unicast-only","text":"Modern applications use IP based communication. In the early days of networking there were also other protocols like IPX or AppleTalk. Thankfully those days are over. Still, we see some applications, that use features from the Ethernet layer, that most IP networks are based on. A typical use case is cache invalidation, that a Java application triggers via UDP broadcasts to its neighbors (see Java Caching System ). In datacenter networking and especially virtual networking so called BUM traffic creates a lot of pain for the operator. Also modern queueing and pubsub solutions came up: ActiveMQ, RabbitMQ, Kafka, NATS. Distributing information using such queueing system is much more comfortable and reliable. We think Ethernet based networking does not fit into the Cloud Native world! You should use IP-based protocols instead! By removing Ethernet networks from Gardener on Metal we save lots of complexity in our infrastructure. All networking is IP based and every packet will be routed. All traffic coming from a VM or a bare metal machine will be routed by the hypervisor or a SmartNIC to the target. There is no Ethernet connectivity between two machines but an IP network. A VM has its hypervisor configured as a router: # ip route show default via 169.254.0.1 dev eno0 proto dhcp src 233.252.0.103 metric 1024 169.254.0.1/32 dev eno0 proto dhcp scope link src 233.252.0.103 metric 1024 # ip -6 route show default via fe80::1 dev eno0 proto dhcpv6 src 2001:db8::1 metric 256 pref medium fe80::/64 dev eno0 proto kernel metric 256 pref medium We reduce complexity and also increase resiliency a lot by avoiding BUM traffic.","title":"Layer 3 unicast only"},{"location":"network/idea/#ipv6-underlay","text":"The Gardener on Metal underlay network uses IPv6 only. We have no need to support IPv4 in the underlay. All customer traffic is encapsulated in IPv6 and sent through our underlay network. IPv6 helps structuring the address space and uses globally unique addresses. No need to use the same IPv4 address space multiple times and trying to find unused address ranges.","title":"IPv6 underlay"},{"location":"network/networkfunctions/","text":"Network Functions \u00b6 Scalable NAT Gateway \u00b6 A horizontally scalable NAT gateway will be implemented. The target is to dedicate one (or more) public IPv4 address for every tenant. Different tenants must not share the same NAT gateway IP. We reduce the amount of required public IPv4 addresses by using the same address on multiple hypervisors. In the drawing you can see three VMs of the same tenant sharing one public IPv4 NAT gateway address. VM B, residing on Hypervisor 2 (HV2), establishes an outgoing connection. The outgoing packets (green) will be SNATed to src IP 45.86.152.23 and src-port 2345. HV2 assigns src ports between 2048 and 3091 to NATed packets. This restricts the number of NATed connections for VM B to the same target IP to 1024. HV2 will send the outgoing packet via the router to the target. Responses (blue) will ingress the cluster through the router. The router encaps the packets into the tenant\u2019s overlay network. It uses ECMP to send the packets to the Hypervisors. The router does not know the correct target hypervisor as it does not base its routing decisions on Layer 4 information but only on IP addresses. In this example the incoming packet (dst ip: 45.86.152.23, dst port: 2345) hits HV1. HV1 is only responsible for the port range 1024-2047. According to a map it redirects the incoming packet to HV2. HV2 receives the packet, finds an entry in its conntrack table and NATs it to VM B. To increase the number of connections per VM the port ranges can be increased. When all 65535 ports of one IP address are used (because of number of VMs and number of connections per VM), a second IP address can be added. This allows to scale the NAT gateway horizontally. Virtual IP Address \u00b6 To realize virtual IP addresses (AWS calls them Elastic IP addresses), DNAT is used. The VM will only know about its private IP address. The virtual IP address will be translated to the VM\u2019s private IP address. This will be done for IPv4 as well as for IPv6. Load Balancer \u00b6 This concept is inspired by Google\u2019s Maglev paper (https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44824.pdf). VM A (10.0.0.12) and VM B (10.0.0.47) each run an https server, that will be L4 load balanced and exposed via 45.86.152.23:443. Packet flows must be sticky to not drop TCP sessions in case of re-routing or when scaling the load balancer or the number of https servers. Incoming packets will be routed using ECMP to the load balancer nodes. As long as the number of load balancers does not change, the packets of a 5-tuple flow will hit the same load balancer node. As soon as the number of load balancers change the ECMP hash buckets will be shuffled and the packets of a TCP flow will hit different load balancer nodes. All load balancer nodes use the same hashing algorithm to distribute the traffic over the https servers. Additionally they remember flows according to its 5-tuple and do not look up the target for every single packet but only for the first packet of a flow. So even if the number of https servers changes, active flows will still hit the same https server. This allows to either scale the load balancer nodes or the https servers without dropping TCP sessions.","title":"Network Functions"},{"location":"network/networkfunctions/#network-functions","text":"","title":"Network Functions"},{"location":"network/networkfunctions/#scalable-nat-gateway","text":"A horizontally scalable NAT gateway will be implemented. The target is to dedicate one (or more) public IPv4 address for every tenant. Different tenants must not share the same NAT gateway IP. We reduce the amount of required public IPv4 addresses by using the same address on multiple hypervisors. In the drawing you can see three VMs of the same tenant sharing one public IPv4 NAT gateway address. VM B, residing on Hypervisor 2 (HV2), establishes an outgoing connection. The outgoing packets (green) will be SNATed to src IP 45.86.152.23 and src-port 2345. HV2 assigns src ports between 2048 and 3091 to NATed packets. This restricts the number of NATed connections for VM B to the same target IP to 1024. HV2 will send the outgoing packet via the router to the target. Responses (blue) will ingress the cluster through the router. The router encaps the packets into the tenant\u2019s overlay network. It uses ECMP to send the packets to the Hypervisors. The router does not know the correct target hypervisor as it does not base its routing decisions on Layer 4 information but only on IP addresses. In this example the incoming packet (dst ip: 45.86.152.23, dst port: 2345) hits HV1. HV1 is only responsible for the port range 1024-2047. According to a map it redirects the incoming packet to HV2. HV2 receives the packet, finds an entry in its conntrack table and NATs it to VM B. To increase the number of connections per VM the port ranges can be increased. When all 65535 ports of one IP address are used (because of number of VMs and number of connections per VM), a second IP address can be added. This allows to scale the NAT gateway horizontally.","title":"Scalable NAT Gateway"},{"location":"network/networkfunctions/#virtual-ip-address","text":"To realize virtual IP addresses (AWS calls them Elastic IP addresses), DNAT is used. The VM will only know about its private IP address. The virtual IP address will be translated to the VM\u2019s private IP address. This will be done for IPv4 as well as for IPv6.","title":"Virtual IP Address"},{"location":"network/networkfunctions/#load-balancer","text":"This concept is inspired by Google\u2019s Maglev paper (https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44824.pdf). VM A (10.0.0.12) and VM B (10.0.0.47) each run an https server, that will be L4 load balanced and exposed via 45.86.152.23:443. Packet flows must be sticky to not drop TCP sessions in case of re-routing or when scaling the load balancer or the number of https servers. Incoming packets will be routed using ECMP to the load balancer nodes. As long as the number of load balancers does not change, the packets of a 5-tuple flow will hit the same load balancer node. As soon as the number of load balancers change the ECMP hash buckets will be shuffled and the packets of a TCP flow will hit different load balancer nodes. All load balancer nodes use the same hashing algorithm to distribute the traffic over the https servers. Additionally they remember flows according to its 5-tuple and do not look up the target for every single packet but only for the first packet of a flow. So even if the number of https servers changes, active flows will still hit the same https server. This allows to either scale the load balancer nodes or the https servers without dropping TCP sessions.","title":"Load Balancer"},{"location":"network/nics/","text":"NIC Offloading \u00b6 For bare metal deployments Gardener on Metal requires NICs, that have their own little operating system on board and can be isolated from the host system. Often those NICs are called SmartNICs or DPUs (intel calls them IPUs). A typical, widely available SmartNIC is the NVIDIA Bluefield-2 . Intel recently has announced Mt. Evans . For VM deployments a modern NIC with DPDK capabilities is required. A broad set of hardware offloading capabilities is recommended. Good candidates are NVIDIA ConnectX-6 Dx or intel E810 .","title":"NIC Offloading"},{"location":"network/nics/#nic-offloading","text":"For bare metal deployments Gardener on Metal requires NICs, that have their own little operating system on board and can be isolated from the host system. Often those NICs are called SmartNICs or DPUs (intel calls them IPUs). A typical, widely available SmartNIC is the NVIDIA Bluefield-2 . Intel recently has announced Mt. Evans . For VM deployments a modern NIC with DPDK capabilities is required. A broad set of hardware offloading capabilities is recommended. Good candidates are NVIDIA ConnectX-6 Dx or intel E810 .","title":"NIC Offloading"},{"location":"network/routing/","text":"Routing \u00b6 Introduction \u00b6 VNet \u00b6 Every VNet has one routing table. A VNet is identified by a 24 bit Virtual Network Identifier (VNI). A VNet can have multiple subnets. IPv4 vs. IPv6 \u00b6 Gardener on Metal's underlay network is an IPv6-only network. The overlay network supports IPv4/IPv6 dual stack. In this document mostly IPv4 addresses are used for the overlay. If nothing different is mentioned, the same techniques apply also to IPv6! Scenarios \u00b6 Routing within a customer network \u00b6 Customer VM AZ Hypervisor VNI Private IPv4 Public IPv4 A VM A.1 FRA3 SRV C5.FRA3 10 10.0.0.1/32 233.252.0.103/32 A VM A.2 FRA3 SRV C6.FRA3 10 10.0.0.2/32 203.0.113.12/32 A VM A.3 FRA4 SRV C2.FRA4 10 10.0.0.3/32 203.0.113.47/32 Hypervisor AZ IPv6 SRV C5.FRA3 FRA3 2001:db8:f3:5::/64 SRV C6.FRA3 FRA3 2001:db8:f3:6::/64 SRV C2.FRA4 FRA4 2001:db8:f4:2::/64 VM A.1 sends a ping request to VM A.2 . This ping packet has an IPv4 header with src IPv4: 10.0.0.1 and dst IPv4: 10.0.0.2 . The network hypervisor on SRV C5.FRA3 , where VM A.1 is running on, will encapsulate the packet. The encapsulation header will hold the following information: dst IPv6: 2001:db8:f3:6:: , VNI: 10 , Protocol: 0x0800 (IPv4) . In case of SRv6 encapsulation the VNI (10 = 0xa) would move into the dst IPv6 address: SRv6 segment: 2001:db8:f3:6:a:: . The SRv6 segment address is provided via the routing control plane and MUST NOT be calculated by the packet's sender using the Host IP and VNI. This is the simplest scenario of routing. To collect the information needed for encapsulating the packet, the hypervisor needs to receive all routes and all route updates of VNet 10. As soon as a new machine is joining VNet 10 or a machine gets terminated, route updates must be delivered to all hypervisors, that host VMs connected to VNet 10. This can be done via Kubernetes watches or, more performant, using a PubSub mechanism, that provides a queue per VNI. Route Updates will be published via this queue to all subscribers. Using the route updates the hypervisor is able to create a routing table for VNet 10. The hypervisor will do longest prefix matching (LPM) for the target IP of the outgoing packet (here 10.0.0.2 ) in the VM's respective VNI context. When VM A.1 sends a ping to VM A.3 in a different AZ, the same happens again. The underlay networks of different AZs of the same region are routed - and also the routing information in the overlay networks is shared. The hypervisor would simply send the encapsulated packet to SRV C2.FRA4 , which is located in FRA4 AZ. Also, when a public IP is used as the destination IP, everything stays the same. Routing information of public endpoints, which reside in VNet 10, are distributed in the same way like private endpoints. When we look at IPv6, there are basically no \"private\" IP addresses anymore. Every IPv6 address is globally unique. But it is possible to not route them to the public internet - and by that make them \"private\". Unequal Cost Multi Path - UCMP \u00b6 Two or more VMs may serve the same IP addresses. In this case there exist multiple routes for a given destination. Every route has a weight attached (0-255). According to the routes' weights outgoing flows will be assigned to the routes. E.g. if we have two routes to 10.0.0.2/32 , one with weight 100 and the other with weight 50, the first route will get assigned to 100/(100+50) = 2/3 of the new flows and the second route will be used for 50/(100+50) = 1/3 of the flows. Routing to the Internet \u00b6 Customer VM AZ Hypervisor VNI Private IPv4 Public IPv4 A VM A.1 FRA3 SRV C5.FRA3 10 10.0.0.1/32 233.252.0.103/32 Hypervisor AZ IPv6 SRV C5.FRA3 FRA3 2001:db8:f3:5::/64 Router-1 FRA3 2001:db8:1::/64 Router-2 FRA3 2001:db8:2::/64 Router-3 FRA4 2001:db8:3::/64 Router-4 FRA4 2001:db8:4::/64 VM A.1 sends a ping request to 8.8.8.8 using its public IPv4 address 233.252.0.103 . The hypervisor does LPM for the target IP. As this IP address is outside of the Gardener on Metal installation, the longest prefix that can be found is the default route 0.0.0.0/0 . This route can be found within the VNet 10 routing context, but it comes with a different target VNI: 1. The VNI 1 is reserved for external internet traffic. All traffic, that is coming from a VM going to the public internet, will be encapsulated using VNI 1. In VNet 10's routing table two routes are installed to serve the default route. The first route is via Router-1 ( 0.0.0.0/0 via 2001:db8:1:: VNI 1 - loc FRA3 - weight 100 ) and the second via Router-2 ( 0.0.0.0/0 via 2001:db8:2:: VNI 1 - loc FRA3 - weight 100 ). Both routes will be used respective to their weights using UCMP. Router-3 and Router-4 are located in FRA4 AZ. This will also be indicated in the route: Router-3 announces 0.0.0.0/0 via 2001:db8:3:: VNI 1 - loc FRA4 - weight 100 ). The client MUST prefer routes that align with their own loc parameter (here FRA3). How will the VNet 10 routing table be filled with default routes from VNet 1? We do not distribute the information about the default routes via the VNI 10 queue. We would need to do this for every customer's VNet, which would require publishing route updates to multiple hundreds or thousands of queues (remember, every VNet has its own queue). Instead the route updates will be published via the VNI 1 queue only. The hypervisor is also always subscribed to the VNI 1 queue and imports the routes received via VNI 1 queue to the VNI 10 routing context. This way route updates of VNI 1 need only be sent to the queue once, but will be received by all hypervisors. The router needs to know where to deliver the Ping response to. We do not want the router to subscribe to all VNI queues that exist. Therefore the route information about the public endpoints (here 233.252.0.103/32 via 2001:db8:f3:5:: VNI 10 - loc FRA3 - weight 100 ) will also be published to the VNI 2 queue. The routing table of VNet 2 will contain all publicly accessible endpoints of all customers. The routers will import the routes of VNet 2 and by that know where to route incoming traffic. Also the hypervisors may import the routes from VNet 2. This would allow communication via public endpoints between different customers. The downside is, this results in larger routing tables on every hypervisor. If this appears as not feasible, the hypervisors will not import VNet 2 routes, but only the default routes from VNet 1. Then all traffic will go to the routers and from there to the other customer. VNet Peering \u00b6 VNet Peering describes the routing between two different virtual networks. We already learned the technical principles behind VNet Peering when we looked at Routing to the Internet: Importing of routes of a different VNet. The use case here is the following: Customer A and customer B want to set up a private connection between their infrastructures. VM A.1 should be able to ping VM B.2 and vice versa. VNet Peering works by importing routes from the neighbor's VNet. Customer VM AZ Hypervisor VNI Private IPv4 A VM A.1 FRA3 SRV C5.FRA3 10 10.0.0.1/32 A VM A.3 FRA3 SRV C4.FRA3 10 172.16.0.3/32 B VM B.2 FRA3 SRV C6.FRA3 47 192.168.0.2/32 B VM B.3 FRA3 SRV C1.FRA3 47 172.16.0.3/32 Hypervisor AZ IPv6 SRV C1.FRA3 FRA4 2001:db8:f3:1::/64 SRV C4.FRA3 FRA4 2001:db8:f3:4::/64 SRV C5.FRA3 FRA4 2001:db8:f3:5::/64 SRV C6.FRA3 FRA4 2001:db8:f3:6::/64 Customer VNI Subnet A 10 10.0.0.0/8 A 10 172.16.0.0/16 B 47 192.168.0.0/24 B 47 172.16.0.0/16 When establishing a peering connectivity between two VNets the customers can decide if they want to import all foreign routes or only routes with a specific prefix. In this example we have two customers A (VNI 10) and B (VNI 47). Customer A is using the subnet 10.0.0.0/0 (VM A.1) and 172.16.0.0/16 (VM A.3). Customer B uses the subnets 192.168.0.0/24 (VM B.2) and 172.16.0.0/16 (VM B.3). As you can see we have partly overlapping IP spaces: 172.16.0.0/16 is used by both customers. As a result we cannot import the routes from VNet 47 (customer B) with the prefix 172.16.0.0/16 into the VNet 10 (customer A). Still, we can import VNet 47 routes with the prefix 192.168.0.0/24 to VNet 10. So in this case it is required to not import all routes of VNet 47 into VNet 10 but use a filter for 192.168.0.0/24 . To also have a route from VNet 47 to VNet 10 we need to import the 10.0.0.0/8 prefixed routes from VNet 10 into VNet 47. As a result the routing table of VNet 10 looks like this: Destination VNI via Loc Weight Remark 10.0.0.1/32 10 2001:db8:f3:5:: FRA3 100 172.16.0.3/32 10 2001:db8:f3:4:: FRA3 100 192.168.0.2/32 47 2001:db8:f3:6:: FRA3 100 imported from VNet 47 And the routing table of VNet 47 looks like this: Destination VNI via Loc Weight Remark 10.0.0.1/32 10 2001:db8:f3:5:: FRA3 100 imported from VNet 10 172.16.0.3/32 47 2001:db8:f3:1:: FRA3 100 192.168.0.2/32 47 2001:db8:f3:6:: FRA3 100","title":"Routing"},{"location":"network/routing/#routing","text":"","title":"Routing"},{"location":"network/routing/#introduction","text":"","title":"Introduction"},{"location":"network/routing/#vnet","text":"Every VNet has one routing table. A VNet is identified by a 24 bit Virtual Network Identifier (VNI). A VNet can have multiple subnets.","title":"VNet"},{"location":"network/routing/#ipv4-vs-ipv6","text":"Gardener on Metal's underlay network is an IPv6-only network. The overlay network supports IPv4/IPv6 dual stack. In this document mostly IPv4 addresses are used for the overlay. If nothing different is mentioned, the same techniques apply also to IPv6!","title":"IPv4 vs. IPv6"},{"location":"network/routing/#scenarios","text":"","title":"Scenarios"},{"location":"network/routing/#routing-within-a-customer-network","text":"Customer VM AZ Hypervisor VNI Private IPv4 Public IPv4 A VM A.1 FRA3 SRV C5.FRA3 10 10.0.0.1/32 233.252.0.103/32 A VM A.2 FRA3 SRV C6.FRA3 10 10.0.0.2/32 203.0.113.12/32 A VM A.3 FRA4 SRV C2.FRA4 10 10.0.0.3/32 203.0.113.47/32 Hypervisor AZ IPv6 SRV C5.FRA3 FRA3 2001:db8:f3:5::/64 SRV C6.FRA3 FRA3 2001:db8:f3:6::/64 SRV C2.FRA4 FRA4 2001:db8:f4:2::/64 VM A.1 sends a ping request to VM A.2 . This ping packet has an IPv4 header with src IPv4: 10.0.0.1 and dst IPv4: 10.0.0.2 . The network hypervisor on SRV C5.FRA3 , where VM A.1 is running on, will encapsulate the packet. The encapsulation header will hold the following information: dst IPv6: 2001:db8:f3:6:: , VNI: 10 , Protocol: 0x0800 (IPv4) . In case of SRv6 encapsulation the VNI (10 = 0xa) would move into the dst IPv6 address: SRv6 segment: 2001:db8:f3:6:a:: . The SRv6 segment address is provided via the routing control plane and MUST NOT be calculated by the packet's sender using the Host IP and VNI. This is the simplest scenario of routing. To collect the information needed for encapsulating the packet, the hypervisor needs to receive all routes and all route updates of VNet 10. As soon as a new machine is joining VNet 10 or a machine gets terminated, route updates must be delivered to all hypervisors, that host VMs connected to VNet 10. This can be done via Kubernetes watches or, more performant, using a PubSub mechanism, that provides a queue per VNI. Route Updates will be published via this queue to all subscribers. Using the route updates the hypervisor is able to create a routing table for VNet 10. The hypervisor will do longest prefix matching (LPM) for the target IP of the outgoing packet (here 10.0.0.2 ) in the VM's respective VNI context. When VM A.1 sends a ping to VM A.3 in a different AZ, the same happens again. The underlay networks of different AZs of the same region are routed - and also the routing information in the overlay networks is shared. The hypervisor would simply send the encapsulated packet to SRV C2.FRA4 , which is located in FRA4 AZ. Also, when a public IP is used as the destination IP, everything stays the same. Routing information of public endpoints, which reside in VNet 10, are distributed in the same way like private endpoints. When we look at IPv6, there are basically no \"private\" IP addresses anymore. Every IPv6 address is globally unique. But it is possible to not route them to the public internet - and by that make them \"private\".","title":"Routing within a customer network"},{"location":"network/routing/#unequal-cost-multi-path-ucmp","text":"Two or more VMs may serve the same IP addresses. In this case there exist multiple routes for a given destination. Every route has a weight attached (0-255). According to the routes' weights outgoing flows will be assigned to the routes. E.g. if we have two routes to 10.0.0.2/32 , one with weight 100 and the other with weight 50, the first route will get assigned to 100/(100+50) = 2/3 of the new flows and the second route will be used for 50/(100+50) = 1/3 of the flows.","title":"Unequal Cost Multi Path - UCMP"},{"location":"network/routing/#routing-to-the-internet","text":"Customer VM AZ Hypervisor VNI Private IPv4 Public IPv4 A VM A.1 FRA3 SRV C5.FRA3 10 10.0.0.1/32 233.252.0.103/32 Hypervisor AZ IPv6 SRV C5.FRA3 FRA3 2001:db8:f3:5::/64 Router-1 FRA3 2001:db8:1::/64 Router-2 FRA3 2001:db8:2::/64 Router-3 FRA4 2001:db8:3::/64 Router-4 FRA4 2001:db8:4::/64 VM A.1 sends a ping request to 8.8.8.8 using its public IPv4 address 233.252.0.103 . The hypervisor does LPM for the target IP. As this IP address is outside of the Gardener on Metal installation, the longest prefix that can be found is the default route 0.0.0.0/0 . This route can be found within the VNet 10 routing context, but it comes with a different target VNI: 1. The VNI 1 is reserved for external internet traffic. All traffic, that is coming from a VM going to the public internet, will be encapsulated using VNI 1. In VNet 10's routing table two routes are installed to serve the default route. The first route is via Router-1 ( 0.0.0.0/0 via 2001:db8:1:: VNI 1 - loc FRA3 - weight 100 ) and the second via Router-2 ( 0.0.0.0/0 via 2001:db8:2:: VNI 1 - loc FRA3 - weight 100 ). Both routes will be used respective to their weights using UCMP. Router-3 and Router-4 are located in FRA4 AZ. This will also be indicated in the route: Router-3 announces 0.0.0.0/0 via 2001:db8:3:: VNI 1 - loc FRA4 - weight 100 ). The client MUST prefer routes that align with their own loc parameter (here FRA3). How will the VNet 10 routing table be filled with default routes from VNet 1? We do not distribute the information about the default routes via the VNI 10 queue. We would need to do this for every customer's VNet, which would require publishing route updates to multiple hundreds or thousands of queues (remember, every VNet has its own queue). Instead the route updates will be published via the VNI 1 queue only. The hypervisor is also always subscribed to the VNI 1 queue and imports the routes received via VNI 1 queue to the VNI 10 routing context. This way route updates of VNI 1 need only be sent to the queue once, but will be received by all hypervisors. The router needs to know where to deliver the Ping response to. We do not want the router to subscribe to all VNI queues that exist. Therefore the route information about the public endpoints (here 233.252.0.103/32 via 2001:db8:f3:5:: VNI 10 - loc FRA3 - weight 100 ) will also be published to the VNI 2 queue. The routing table of VNet 2 will contain all publicly accessible endpoints of all customers. The routers will import the routes of VNet 2 and by that know where to route incoming traffic. Also the hypervisors may import the routes from VNet 2. This would allow communication via public endpoints between different customers. The downside is, this results in larger routing tables on every hypervisor. If this appears as not feasible, the hypervisors will not import VNet 2 routes, but only the default routes from VNet 1. Then all traffic will go to the routers and from there to the other customer.","title":"Routing to the Internet"},{"location":"network/routing/#vnet-peering","text":"VNet Peering describes the routing between two different virtual networks. We already learned the technical principles behind VNet Peering when we looked at Routing to the Internet: Importing of routes of a different VNet. The use case here is the following: Customer A and customer B want to set up a private connection between their infrastructures. VM A.1 should be able to ping VM B.2 and vice versa. VNet Peering works by importing routes from the neighbor's VNet. Customer VM AZ Hypervisor VNI Private IPv4 A VM A.1 FRA3 SRV C5.FRA3 10 10.0.0.1/32 A VM A.3 FRA3 SRV C4.FRA3 10 172.16.0.3/32 B VM B.2 FRA3 SRV C6.FRA3 47 192.168.0.2/32 B VM B.3 FRA3 SRV C1.FRA3 47 172.16.0.3/32 Hypervisor AZ IPv6 SRV C1.FRA3 FRA4 2001:db8:f3:1::/64 SRV C4.FRA3 FRA4 2001:db8:f3:4::/64 SRV C5.FRA3 FRA4 2001:db8:f3:5::/64 SRV C6.FRA3 FRA4 2001:db8:f3:6::/64 Customer VNI Subnet A 10 10.0.0.0/8 A 10 172.16.0.0/16 B 47 192.168.0.0/24 B 47 172.16.0.0/16 When establishing a peering connectivity between two VNets the customers can decide if they want to import all foreign routes or only routes with a specific prefix. In this example we have two customers A (VNI 10) and B (VNI 47). Customer A is using the subnet 10.0.0.0/0 (VM A.1) and 172.16.0.0/16 (VM A.3). Customer B uses the subnets 192.168.0.0/24 (VM B.2) and 172.16.0.0/16 (VM B.3). As you can see we have partly overlapping IP spaces: 172.16.0.0/16 is used by both customers. As a result we cannot import the routes from VNet 47 (customer B) with the prefix 172.16.0.0/16 into the VNet 10 (customer A). Still, we can import VNet 47 routes with the prefix 192.168.0.0/24 to VNet 10. So in this case it is required to not import all routes of VNet 47 into VNet 10 but use a filter for 192.168.0.0/24 . To also have a route from VNet 47 to VNet 10 we need to import the 10.0.0.0/8 prefixed routes from VNet 10 into VNet 47. As a result the routing table of VNet 10 looks like this: Destination VNI via Loc Weight Remark 10.0.0.1/32 10 2001:db8:f3:5:: FRA3 100 172.16.0.3/32 10 2001:db8:f3:4:: FRA3 100 192.168.0.2/32 47 2001:db8:f3:6:: FRA3 100 imported from VNet 47 And the routing table of VNet 47 looks like this: Destination VNI via Loc Weight Remark 10.0.0.1/32 10 2001:db8:f3:5:: FRA3 100 imported from VNet 10 172.16.0.3/32 47 2001:db8:f3:1:: FRA3 100 192.168.0.2/32 47 2001:db8:f3:6:: FRA3 100","title":"VNet Peering"}]}